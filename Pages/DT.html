<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Climate penalty</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="../assets/img/favicon.png" rel="icon">
  <link href="../assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Roboto:300,300i,400,400i,500,500i,700,700i&display=swap" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="../assets/vendor/animate.css/animate.min.css" rel="stylesheet">
  <link href="../assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="../assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="../assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="../assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="../assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="../assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="../assets/css/style.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: Moderna - v4.11.0
  * Template URL: https://bootstrapmade.com/free-bootstrap-template-corporate-moderna/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top ">
    <div class="container d-flex align-items-center justify-content-lg-between">

      <h1 class="text-light"><a href="../index.html"><span style="color: white; font-size: 25px;">Avinya</span></a></h1>
      <!-- Uncomment below if you prefer to use an image logo -->
      <!-- <a href="index.html" class="logo me-auto me-lg-0"><img src="../assets/img/logo.png" alt="" class="img-fluid"></a>-->

      <nav id="navbar" class="navbar">
        <ul>
          <li><a  href="../index.html">Home</a></li>
          <li><a href="./EDA.html">EDA</a></li>
         
          <li class="dropdown"><a  href="#"><span>Unsupervised Learning</span> <i class="bi bi-chevron-down"></i></a>
            <ul>
              <li><a href="./Clustering.html">Clustering</a></li>
              <li><a href="./ARM.html">ARM</a></li>
            </ul>
          </li>
          <li class="dropdown"><a class="active"  href="#"><span>Supervised Learning</span> <i class="bi bi-chevron-down"></i></a>
            <ul>
              <li><a href="./DT.html">Decision Trees</a></li>
              
              <li><a href="./NB.html">Naive Bayes</a></li>
              <li><a href="./SVM.html">SVM</a></li>
            </ul>
          </li>
          <li><a  href="./LDA.html">LDA</a></li>

          <li><a  href="./NN.html">Neural Networks</a></li>
          <li><a  href="../conc.html">Conclusion</a></li>
          
          
        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav><!-- .navbar -->
      

    </div>
  </header><!-- End Header -->




  
  <main id="main">
     <!-- ======= About Us Section ======= -->
     <section class="breadcrumbs">
      <div class="container">

        <div class="d-flex justify-content-between align-items-center">
          <h2>Decision Tree</h2>
          <ol>
            <li><a href="../index.html">Home</a></li>
            <li>Decision Tree</li>
          </ol>
        </div>

      </div>
    </section><!-- End About Us Section -->
   
    <!-- ======= Features Section ======= -->
   <!-- 
  
  Radio version of tabs.

  Requirements:
  - not rely on specific IDs for CSS (the CSS shouldn't need to know specific IDs)
  - flexible for any number of unkown tabs [2-6]
  - accessible

  Caveats:
  - since these are checkboxes the tabs not tab-able, need to use arrow keys

  Also worth reading:
  http://simplyaccessible.com/article/danger-aria-tabs/
-->

<div class="tabset">
  <!-- Tab 1 -->
  <input type="radio" name="tabset" id="tab1" aria-controls="marzen" checked>
  <label for="tab1">Introduction</label>
  <!-- Tab 2 -->
  <input type="radio" name="tabset" id="tab2" aria-controls="rauchbier">
  <label for="tab2">Data Gathering</label>
  <!-- Tab 3 -->
  <input type="radio" name="tabset" id="tab3" aria-controls="dunkles">
  <label for="tab3">Results & Conclusion </label>
  
  <div class="tab-panels">
    <section id="marzen" class="tab-panel">
      
        <div class="container" style="max-width: 100%;">
  
          <div class="row" >
            <div class="col-md-5">
              <img src="../assets/img/DT/dt2.png" class="img-fluid" alt="" style="height: 270px; width: 700px;">
            </div>
            <div class="col-md-7">
              <p style="font-size: 20px;font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif, Courier, monospace; padding: 10px;">
                A decision tree is a machine learning algorithm that can be used for both classification and regression tasks. It is a type of model that makes predictions by learning simple decision rules from data. A decision tree can be seen as a flowchart-like structure, where each internal node represents a "test" on a feature or attribute, each branch represents the outcome of the test, and each leaf node represents a class label or a numerical value that is the result of the decision tree's prediction.

                Decision trees are often used in data mining and predictive analytics applications, as they are easy to interpret and can be used for both categorical and numerical data. They can also handle both continuous and discrete variables, making them very versatile.
                
                </p>
             
            </div>
            <p style="font-size: 20px;font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif, Courier, monospace; padding: 10px;">
              Some common applications of decision trees include:
              
              Customer segmentation: Decision trees can be used to group customers based on their behavior, preferences, and demographics, allowing businesses to tailor their marketing and product offerings to specific customer segments.
              
              Fraud detection: Decision trees can be used to detect fraudulent transactions by identifying patterns in data that indicate fraud.
              
              Medical diagnosis: Decision trees can be used to diagnose medical conditions based on symptoms and patient history.
              
              Predictive maintenance: Decision trees can be used to predict when equipment or machinery is likely to fail, allowing maintenance teams to proactively schedule repairs and prevent downtime.
              
              Overall, decision trees are a powerful tool for making predictions based on complex data sets, and they have a wide range of applications across industries.
    
              
            </p>
          </div>
  
          <div class="row" data-aos="fade-up">
            <div class="col-md-5 order-1 order-md-2">
              <img src="../assets/img/DT/dt1.webp" class="img-fluid" alt="" style="height: 200px">
              <br/><br/>
              <img src="../assets/img/DT/dt3.jpeg" class="img-fluid" alt="" style="height: 300px">
            </div>
            <div class="col-md-7 pt-5 order-2 order-md-1">
              
            
              <p style="font-size: 20px;font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif, Courier, monospace; padding: 10px;">
                When building decision trees, we need a way to determine which features or attributes to split on, and which ones are the most informative for making predictions. GINI, Entropy, and Information Gain are three popular measures used to evaluate the quality of a split in a decision tree.

GINI Impurity: GINI is a measure of impurity or randomness used to evaluate the quality of a binary split. It calculates the probability of misclassification if a random sample is classified according to the distribution of target classes in the split. A GINI score of 0 means that all samples in a node belong to the same class, while a score of 0.5 means that the samples are equally distributed among all classes.

Entropy: Entropy is another measure of impurity or randomness used to evaluate the quality of a binary split. It calculates the amount of uncertainty or disorder in a system, which in this case, is the distribution of target classes in the split. A split with low entropy means that the target classes are mostly the same, while a split with high entropy means that the target classes are distributed more evenly.

Information Gain: Information Gain is a measure of how much information is gained by splitting on a particular feature or attribute. It calculates the difference in entropy or GINI impurity between the parent node and the child nodes resulting from the split. A higher information gain means that the split is more informative and should be preferred over other splits.
              </p>
            </div>
            <hr/>
          
         
  
        </div>
      </section><!-- End Features Section -->
    <section id="rauchbier" class="tab-panel">
     <ol>
      <li style="font-size: 20px;font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif, Courier, monospace; padding: 10px;">Used Clustering data: ClusterData_Twitter <br/>
        Link to the dataset: <a href="https://github.com/Sreepa1995/avinya.github.io/blob/main/data/DTData/ClusterData_Twitter.csv">Dataset</a>
        <br/> Link to the Python code: <a href="https://github.com/Sreepa1995/avinya.github.io/blob/main/code/Python/TextMining_ProjectV1.ipynb">Code</a>

        <br/>
      <div class="row">
        <div class="col-lg-6">
          <img src="../assets/img/DT1/sample.jpg" class="img-fluid" alt="" style="height: 300px;">
         
          </div>
          <div class="col-lg-6 pt-4 pt-lg-0">
           
            <img src="../assets/img/DT1/sample2.jpg" class="img-fluid" alt="" style="height: 300px">
        </div> 
          <p style="font-size: 20px;font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif, Courier, monospace; padding: 10px;">
            
            Decision tree could be constructed using clinical trial data to predict the effectiveness of omega-3 supplements in reducing inflammation in patients with rheumatoid arthritis. The decision tree could use input features such as the dosage of omega-3 supplements, the duration of use, and the presence of pre-existing medical conditions such as cardiovascular disease or diabetes. The decision tree would then predict the effectiveness of omega-3 supplements for reducing inflammation based on the combination of input features.

However, it's important to note that decision trees are not always the most appropriate algorithm for every type of problem or dataset. In the case of supplement intake, there may be many complex and interacting factors that influence the effectiveness and safety of supplements that are difficult to capture in a decision tree model. Therefore, other supervised learning algorithms such as random forests or support vector machines may be more appropriate for some types of supplement intake analysis.
          </p>
     <br/>

    
    </ol>
    </section>
    <section id="dunkles" class="tab-panel">
      <h4>Results</h4>
      <p style="font-size: 20px;font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif, Courier, monospace; padding: 10px;">
        In this confusion matrix, each row represents the actual label, and each column represents the predicted label. The numbers in the cells show the number of samples that belong to the corresponding true and predicted labels. For example, there are 85 samples that belong to the diet label and were correctly classified as diet, while there are 10 samples that belong to the diet label but were misclassified as protein.

We can use the confusion matrix to calculate several performance metrics for the decision tree model, such as accuracy, precision, recall, and F1 score, for each class label. These metrics provide a more detailed evaluation of the model's performance than the overall accuracy, which can be misleading for imbalanced datasets.

For example, the precision for the diet label is 95%, indicating that 95% of the samples predicted as diet actually belong to the diet class. The recall for the protein label is 92/(92+8) = 0.920, indicating that 92.0% of the actual protein samples were correctly classified as protein by the model.

In summary, a confusion matrix is a useful tool for evaluating the performance of a decision tree model that classifies supplement intake into the labels diet, proteins, drugs, and supplements. The confusion matrix provides a detailed summary of the model's performance for each label, which can help identify areas of improvement and guide further analysis.
        </p>

      <div class="row">
        <b></b>

        <div class="col-lg-6">
          <img src="../assets/img/DT1/dt_cm.png" class="img-fluid" alt="" >
    
        </div>

        <div class="col-lg-6 pt-4 pt-lg-0">
          <img src="../assets/img/DT1/dt_pr.png" class="img-fluid" alt="" >
      </div>
     <br/>
      <hr/>

     
      <p style="font-size: 20px;font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif, Courier, monospace; padding: 10px;">
       
      </p>
      <div class="row">
        <b></b>
        <div class="col-lg-6">
          <img src="../assets/img/DT/gini2.png" class="img-fluid" alt="">
    
        </div>

        <div class="col-lg-6 pt-4 pt-lg-0">
          <img src="../assets/img/DT/gini.png" class="img-fluid" alt="" >
      </div>
      <p style="font-size: 20px;font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif, Courier, monospace; padding: 10px;">
        Here is a brief explanation of the arguments passed to the DecisionTreeClassifier constructor:

        criterion: The function to measure the quality of a split. "entropy" is a measure of impurity based on the information theory, while "gini" is a measure based on the Gini impurity.
        
        splitter: The strategy used to choose the split at each node. "random" means that random splits are tried, while "best" means that the best split is chosen.
        
        max_depth: The maximum depth of the decision tree.
        
        min_samples_split: The minimum number of samples required to split an internal node.
        
        min_samples_leaf: The minimum number of samples required to be at a leaf node.
        
        min_weight_fraction_leaf: The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node.
        
        max_features: The maximum number of features to consider when looking for the best split.
        
        random_state: The seed used by the random number generator.
        
        max_leaf_nodes: The maximum number of leaf nodes in the decision tree.
        
        min_impurity_decrease: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.
        
        class_weight: Weights associated with classes in the form {class_label: weight}.
        
        By setting these arguments to specific values, the user can customize the behavior of the decision tree algorithm. Once the instance of DecisionTreeClassifier is created, it can be trained on a labeled dataset using the fit() method and used to predict the class labels of new data points using the predict() method.

   </p>
   <div class="col-lg-6" style="width: 100%;">
    <img src="../assets/img/DT1/tree.jpg" class="img-fluid" alt="" >

  </div>

 
<br/>
<hr/>
<p style="font-size: 20px;font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif, Courier, monospace; padding: 10px;">
  in general, the accuracy score indicates the percentage of correctly classified instances out of all instances in the testing set. A high accuracy score (such as 94% in this case) suggests that the decision tree classifier is performing well on the task of classifying supplement intake with the given labels.

  Precision and recall scores are useful metrics for evaluating the performance of a classifier when dealing with imbalanced datasets. Precision measures the fraction of true positives out of all predicted positives, while recall measures the fraction of true positives out of all actual positives. A high precision score suggests that the classifier is making few false positive predictions, while a high recall score suggests that the classifier is identifying most of the positive instances. The F1 score is a weighted average of precision and recall, and provides a single metric to evaluate the overall performance of the classifier.
  
  The confusion matrix provides a more detailed view of the classifier's performance, showing the number of true positives, false positives, true negatives, and false negatives for each label. This can help identify which labels the classifier is performing well on, and which labels it is struggling with.
  
  To fully understand the results of the decision tree classifier for supplement intake with the labels diet, proteins, drugs, and supplements, we would need to analyze the specific metrics and confusion matrix for each label, as well as the hyperparameters and feature importance of the decision tree classifier.
  
  
  
  </p>
<div class="col-lg-6">
  <img src="../assets/img/DT1/dt_feats.png" class="img-fluid" alt="" >

</div>

<div class="col-lg-6 pt-4 pt-lg-0">
  <img src="../assets/img/DT1/dt_pr_comp.png" class="img-fluid" alt="" >
</div>
<br/>
<hr/>
<p style="font-size: 20px;font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif, Courier, monospace; padding: 10px;">
  By increasing the value of min_samples_split, the decision tree will have fewer nodes and may become less complex. This can help to avoid overfitting, especially when dealing with small datasets or noisy data. However, setting it too high may result in underfitting, where the decision tree is too simple and cannot capture the underlying patterns in the data. Therefore, it's important to carefully choose this hyperparameter based on the specific problem and the characteristics of the dataset.
</p>
<div class="col-lg-6">
  <img src="../assets/img/dt3.png" class="img-fluid" alt="" >

</div>

<div class="col-lg-6 pt-4 pt-lg-0">
  <img src="../assets/img/dt3v3.png" class="img-fluid" alt="" >
</div>
<br/>
<hr/>
  </div>
  
  </section>
</div>



    
  

  </main><!-- End #main -->

 

  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="../assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="../assets/vendor/aos/aos.js"></script>
  <script src="../assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="../assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="../assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="../assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="../assets/vendor/waypoints/noframework.waypoints.js"></script>
  <script src="../assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="../assets/js/main.js"></script>

</body>

</html>